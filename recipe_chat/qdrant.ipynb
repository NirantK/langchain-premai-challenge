{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Cleaned_Ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Miso-Butter Roast Chicken With Acorn Squash Pa...</td>\n",
       "      <td>['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...</td>\n",
       "      <td>Pat chicken dry with paper towels, season all ...</td>\n",
       "      <td>miso-butter-roast-chicken-acorn-squash-panzanella</td>\n",
       "      <td>['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Crispy Salt and Pepper Potatoes</td>\n",
       "      <td>['2 large egg whites', '1 pound new potatoes (...</td>\n",
       "      <td>Preheat oven to 400°F and line a rimmed baking...</td>\n",
       "      <td>crispy-salt-and-pepper-potatoes-dan-kluger</td>\n",
       "      <td>['2 large egg whites', '1 pound new potatoes (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thanksgiving Mac and Cheese</td>\n",
       "      <td>['1 cup evaporated milk', '1 cup whole milk', ...</td>\n",
       "      <td>Place a rack in middle of oven; preheat to 400...</td>\n",
       "      <td>thanksgiving-mac-and-cheese-erick-williams</td>\n",
       "      <td>['1 cup evaporated milk', '1 cup whole milk', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Italian Sausage and Bread Stuffing</td>\n",
       "      <td>['1 (¾- to 1-pound) round Italian loaf, cut in...</td>\n",
       "      <td>Preheat oven to 350°F with rack in middle. Gen...</td>\n",
       "      <td>italian-sausage-and-bread-stuffing-240559</td>\n",
       "      <td>['1 (¾- to 1-pound) round Italian loaf, cut in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Newton's Law</td>\n",
       "      <td>['1 teaspoon dark brown sugar', '1 teaspoon ho...</td>\n",
       "      <td>Stir together brown sugar and hot water in a c...</td>\n",
       "      <td>newtons-law-apple-bourbon-cocktail</td>\n",
       "      <td>['1 teaspoon dark brown sugar', '1 teaspoon ho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title  \\\n",
       "0           0  Miso-Butter Roast Chicken With Acorn Squash Pa...   \n",
       "1           1                    Crispy Salt and Pepper Potatoes   \n",
       "2           2                        Thanksgiving Mac and Cheese   \n",
       "3           3                 Italian Sausage and Bread Stuffing   \n",
       "4           4                                       Newton's Law   \n",
       "\n",
       "                                         Ingredients  \\\n",
       "0  ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...   \n",
       "1  ['2 large egg whites', '1 pound new potatoes (...   \n",
       "2  ['1 cup evaporated milk', '1 cup whole milk', ...   \n",
       "3  ['1 (¾- to 1-pound) round Italian loaf, cut in...   \n",
       "4  ['1 teaspoon dark brown sugar', '1 teaspoon ho...   \n",
       "\n",
       "                                        Instructions  \\\n",
       "0  Pat chicken dry with paper towels, season all ...   \n",
       "1  Preheat oven to 400°F and line a rimmed baking...   \n",
       "2  Place a rack in middle of oven; preheat to 400...   \n",
       "3  Preheat oven to 350°F with rack in middle. Gen...   \n",
       "4  Stir together brown sugar and hot water in a c...   \n",
       "\n",
       "                                          Image_Name  \\\n",
       "0  miso-butter-roast-chicken-acorn-squash-panzanella   \n",
       "1         crispy-salt-and-pepper-potatoes-dan-kluger   \n",
       "2         thanksgiving-mac-and-cheese-erick-williams   \n",
       "3          italian-sausage-and-bread-stuffing-240559   \n",
       "4                 newtons-law-apple-bourbon-cocktail   \n",
       "\n",
       "                                 Cleaned_Ingredients  \n",
       "0  ['1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher...  \n",
       "1  ['2 large egg whites', '1 pound new potatoes (...  \n",
       "2  ['1 cup evaporated milk', '1 cup whole milk', ...  \n",
       "3  ['1 (¾- to 1-pound) round Italian loaf, cut in...  \n",
       "4  ['1 teaspoon dark brown sugar', '1 teaspoon ho...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"data/Food Ingredients and Recipe Dataset with Image Name Mapping.csv\"\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    13493.000000\n",
      "mean      1040.674201\n",
      "std        710.946428\n",
      "min         40.000000\n",
      "25%        569.000000\n",
      "50%        890.000000\n",
      "75%       1345.000000\n",
      "max      13952.000000\n",
      "Name: Instructions_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# compute the length of each instruction\n",
    "data[\"Instructions_length\"] = data[\"Instructions\"].str.len()\n",
    "\n",
    "\n",
    "# print the stats of the column Instructions_length\n",
    "print(data[\"Instructions_length\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "collection_name = \"recipe_title_collection\"\n",
    "vector_dimension = 384\n",
    "\n",
    "qdrant_collections = client.get_collections()\n",
    "\n",
    "# If no collections exist or if the index_name is not present in the collections, create the collection\n",
    "if len(qdrant_collections.collections) == 0 or not any(\n",
    "    collection_name in collection.name for collection in qdrant_collections.collections\n",
    "):\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=vector_dimension, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "    collection_info = client.get_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 27\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m# go through the first 2 rows of the dataset,\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# in the column Instructions, split the text into chunks and add the chunks\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# along with the text in column Title as payload to the list all_points\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[39m# for index, row in data.iloc[:5].iterrows():\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m---> 27\u001b[0m     chunks \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39;49msplit_text(row[\u001b[39m\"\u001b[39;49m\u001b[39mInstructions\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     28\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks:\n\u001b[1;32m     29\u001b[0m         chunk_embedding \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39membed_query(chunk)\n",
      "File \u001b[0;32m~/anaconda3/envs/aditya/lib/python3.11/site-packages/langchain/text_splitter.py:488\u001b[0m, in \u001b[0;36mTokenTextSplitter.split_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mencode(\n\u001b[1;32m    476\u001b[0m         _text,\n\u001b[1;32m    477\u001b[0m         allowed_special\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allowed_special,\n\u001b[1;32m    478\u001b[0m         disallowed_special\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disallowed_special,\n\u001b[1;32m    479\u001b[0m     )\n\u001b[1;32m    481\u001b[0m tokenizer \u001b[39m=\u001b[39m Tokenizer(\n\u001b[1;32m    482\u001b[0m     chunk_overlap\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_overlap,\n\u001b[1;32m    483\u001b[0m     tokens_per_chunk\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_size,\n\u001b[1;32m    484\u001b[0m     decode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mdecode,\n\u001b[1;32m    485\u001b[0m     encode\u001b[39m=\u001b[39m_encode,\n\u001b[1;32m    486\u001b[0m )\n\u001b[0;32m--> 488\u001b[0m \u001b[39mreturn\u001b[39;00m split_text_on_tokens(text\u001b[39m=\u001b[39;49mtext, tokenizer\u001b[39m=\u001b[39;49mtokenizer)\n",
      "File \u001b[0;32m~/anaconda3/envs/aditya/lib/python3.11/site-packages/langchain/text_splitter.py:431\u001b[0m, in \u001b[0;36msplit_text_on_tokens\u001b[0;34m(text, tokenizer)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Split incoming text and return chunks.\"\"\"\u001b[39;00m\n\u001b[1;32m    430\u001b[0m splits: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 431\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(text)\n\u001b[1;32m    432\u001b[0m start_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    433\u001b[0m cur_idx \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(start_idx \u001b[39m+\u001b[39m tokenizer\u001b[39m.\u001b[39mtokens_per_chunk, \u001b[39mlen\u001b[39m(input_ids))\n",
      "File \u001b[0;32m~/anaconda3/envs/aditya/lib/python3.11/site-packages/langchain/text_splitter.py:475\u001b[0m, in \u001b[0;36mTokenTextSplitter.split_text.<locals>._encode\u001b[0;34m(_text)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode\u001b[39m(_text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode(\n\u001b[1;32m    476\u001b[0m         _text,\n\u001b[1;32m    477\u001b[0m         allowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_allowed_special,\n\u001b[1;32m    478\u001b[0m         disallowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_disallowed_special,\n\u001b[1;32m    479\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/aditya/lib/python3.11/site-packages/tiktoken/core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(disallowed_special, \u001b[39mfrozenset\u001b[39m):\n\u001b[1;32m    115\u001b[0m         disallowed_special \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m(disallowed_special)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mif\u001b[39;00m match \u001b[39m:=\u001b[39m _special_token_regex(disallowed_special)\u001b[39m.\u001b[39;49msearch(text):\n\u001b[1;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[39m.\u001b[39mgroup())\n\u001b[1;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from qdrant_client.http.models import PointStruct\n",
    "import uuid\n",
    "\n",
    "# text splitter config\n",
    "text_splitter = TokenTextSplitter(chunk_size=250, chunk_overlap=25)\n",
    "\n",
    "# embedding config - using All MiniLM L6 v2\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/v1\")\n",
    "\n",
    "\n",
    "# vector and payloads as points for qdrant\n",
    "all_points = []\n",
    "\n",
    "\n",
    "# go through the first 2 rows of the dataset,\n",
    "# in the column Instructions, split the text into chunks and add the chunks\n",
    "# along with the text in column Title as payload to the list all_points\n",
    "\n",
    "# for index, row in data.iloc[:5].iterrows():\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    chunks = text_splitter.split_text(row[\"Instructions\"])\n",
    "    for chunk in chunks:\n",
    "        chunk_embedding = embeddings.embed_query(chunk)\n",
    "        vector_id = uuid.uuid4().hex\n",
    "        # print(\"Vector ID: \", vector_id)\n",
    "        all_points.append(\n",
    "            PointStruct(\n",
    "                id=vector_id, vector=chunk_embedding, payload={\"title\": row[\"Title\"]}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # print('Title: ', row['Title'])\n",
    "    # print('Cleaned Ingredients: ', row['Cleaned_Ingredients'])\n",
    "    # print('Chunks: ', chunks)\n",
    "    # print('Total chunks: ', len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a batch upsert of all the points in tranches of 1000\n",
    "for i in range(0, len(all_points), 1000):\n",
    "    print(\"Upserting points from \", i, \" to \", i + 1000)\n",
    "    operation_info = client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        wait=True,\n",
    "        points=all_points[i : i + 1000],\n",
    "    )\n",
    "\n",
    "    from qdrant_client.http.models import UpdateStatus\n",
    "\n",
    "    assert operation_info.status == UpdateStatus.COMPLETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ScoredPoint(id='4dfb2e93-3b60-4846-9669-9a7a957f261a', version=0, score=0.048540186, payload={'title': 'Miso-Butter Roast Chicken With Acorn Squash Panzanella'}, vector=None), ScoredPoint(id='8accaa64-29bd-4aac-b331-5230abba3416', version=0, score=0.034743235, payload={'title': 'Miso-Butter Roast Chicken With Acorn Squash Panzanella'}, vector=None), ScoredPoint(id='5ddfd78a-aed9-4c70-a08e-635eb3c69406', version=0, score=0.026305828, payload={'title': 'Italian Sausage and Bread Stuffing'}, vector=None)]\n"
     ]
    }
   ],
   "source": [
    "# Random Vector - An array of 384 random numbers\n",
    "random_vector = np.random.rand(384)\n",
    "\n",
    "search_result = client.search(\n",
    "    collection_name=collection_name, query_vector=random_vector, limit=3\n",
    ")\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_collection(collection_name=collection_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valmiki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
